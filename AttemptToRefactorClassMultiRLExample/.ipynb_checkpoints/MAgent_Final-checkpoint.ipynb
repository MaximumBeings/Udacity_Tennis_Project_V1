{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reliable-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg3 import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selective-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mental-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fixed-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "egyptian-illinois",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# keep training awake\n",
    "from workspace_utils import keep_awake\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "billion-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rough-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "black-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2 = []\n",
    "def mainA():\n",
    "    seeding()\n",
    "    parallel_envs = 1\n",
    "    number_of_episodes = 100\n",
    "    episode_length = 80\n",
    "    batchsize = 240\n",
    "    save_interval = 100\n",
    "    noise = 2\n",
    "    averageR = []\n",
    "    noise_reduction = 0.9999\n",
    "    episode_per_update = 2 * parallel_envs\n",
    "    log_path = os.getcwd()+\"/log\"\n",
    "    model_dir= os.getcwd()+\"/model_dir\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    torch.set_num_threads(parallel_envs)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states_all = env_info.vector_observations\n",
    "    buffer = ReplayBuffer(int(5000*episode_length))\n",
    "    maddpg = MADDPG()\n",
    "    logger = SummaryWriter(log_dir=log_path)\n",
    "    agent0_reward = []\n",
    "    agent1_reward = []\n",
    "    for episode in (range(0, 10000)):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        reward_this_episode = np.zeros((parallel_envs, 2))\n",
    "        save_info = ((episode) % save_interval < parallel_envs or episode==number_of_episodes-parallel_envs)\n",
    "        \n",
    "        for ag in range(2):\n",
    "            obs_full = env_info.vector_observations #\n",
    "            obs = np.array(obs_full[ag])\n",
    "            actions = maddpg.act(torch.from_numpy(obs_full).float().to(device), noise=noise)\n",
    "            #print(obs_full)\n",
    "            #print(obs)\n",
    "            #for episode_t in range(2):\n",
    "            actions = maddpg.act(torch.from_numpy(obs_full).float().to(device), noise=noise)\n",
    "\n",
    "            noise *= noise_reduction\n",
    "\n",
    "            actions_array = torch.stack(actions).detach().numpy()\n",
    "            actions_for_env = np.rollaxis(actions_array,1)\n",
    "            env_info = env.step(actions_for_env)[brain_name]\n",
    "            next_obs_full = env_info.vector_observations\n",
    "            next_obs = np.array(next_obs_full[ag])\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            transition = (obs, obs_full, actions_for_env, rewards, next_obs, next_obs_full, dones)\n",
    "            buffer.push(transition)\n",
    "            reward_this_episode += rewards\n",
    "            obs, obs_full = next_obs, next_obs_full\n",
    "\n",
    "\n",
    "    \n",
    "        if len(buffer) % batchsize == 0 :\n",
    "                for a_i in range(2):\n",
    "                    samples = buffer.sample(24)\n",
    "                    maddpg.update([samples], a_i, logger)\n",
    "                maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "        for i in range(parallel_envs):\n",
    "            agent0_reward.append(reward_this_episode[i,0])\n",
    "            agent1_reward.append(reward_this_episode[i,1])\n",
    "        \n",
    "        if episode % 100 == 0 or episode == number_of_episodes-1:\n",
    "            avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward)]\n",
    "            max_rewards = [reward_this_episode[i,0], reward_this_episode[i,1]]\n",
    "            averageR.append(max(max_rewards))\n",
    "            agent0_reward = []\n",
    "            agent1_reward = []\n",
    "            \n",
    "            for a_i, avg_rew in enumerate(avg_rewards):\n",
    "                logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "                print('agent%i/mean_episode_rewards' % a_i, np.sum(averageR), episode)\n",
    "                \n",
    "        save_dict_list =[]\n",
    "        if save_info:\n",
    "            for i in range(2):\n",
    "\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "\n",
    "                torch.save(save_dict_list, \n",
    "                           os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #env.close()\n",
    "    logger.close()\n",
    "    #samples = buffer.sample(24)\n",
    "    #maddpg.update([samples], 0, logger)\n",
    "    #maddpg.update_targets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grand-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oluwaseyiawoga/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards 0.0 0\n",
      "agent1/mean_episode_rewards 0.0 0\n",
      "agent0/mean_episode_rewards 0.0 99\n",
      "agent1/mean_episode_rewards 0.0 99\n",
      "agent0/mean_episode_rewards 0.0 100\n",
      "agent1/mean_episode_rewards 0.0 100\n",
      "agent0/mean_episode_rewards 0.0 200\n",
      "agent1/mean_episode_rewards 0.0 200\n",
      "agent0/mean_episode_rewards 0.0 300\n",
      "agent1/mean_episode_rewards 0.0 300\n",
      "agent0/mean_episode_rewards 0.0 400\n",
      "agent1/mean_episode_rewards 0.0 400\n",
      "agent0/mean_episode_rewards 0.0 500\n",
      "agent1/mean_episode_rewards 0.0 500\n",
      "agent0/mean_episode_rewards 0.0 600\n",
      "agent1/mean_episode_rewards 0.0 600\n",
      "agent0/mean_episode_rewards 0.0 700\n",
      "agent1/mean_episode_rewards 0.0 700\n",
      "agent0/mean_episode_rewards 0.0 800\n",
      "agent1/mean_episode_rewards 0.0 800\n",
      "agent0/mean_episode_rewards 0.0 900\n",
      "agent1/mean_episode_rewards 0.0 900\n",
      "agent0/mean_episode_rewards 0.0 1000\n",
      "agent1/mean_episode_rewards 0.0 1000\n",
      "agent0/mean_episode_rewards 0.0 1100\n",
      "agent1/mean_episode_rewards 0.0 1100\n",
      "agent0/mean_episode_rewards 0.0 1200\n",
      "agent1/mean_episode_rewards 0.0 1200\n",
      "agent0/mean_episode_rewards 0.0 1300\n",
      "agent1/mean_episode_rewards 0.0 1300\n",
      "agent0/mean_episode_rewards 0.0 1400\n",
      "agent1/mean_episode_rewards 0.0 1400\n",
      "agent0/mean_episode_rewards 0.0 1500\n",
      "agent1/mean_episode_rewards 0.0 1500\n",
      "agent0/mean_episode_rewards 0.0 1600\n",
      "agent1/mean_episode_rewards 0.0 1600\n",
      "agent0/mean_episode_rewards 0.0 1700\n",
      "agent1/mean_episode_rewards 0.0 1700\n",
      "agent0/mean_episode_rewards 0.0 1800\n",
      "agent1/mean_episode_rewards 0.0 1800\n",
      "agent0/mean_episode_rewards 0.0 1900\n",
      "agent1/mean_episode_rewards 0.0 1900\n",
      "agent0/mean_episode_rewards 0.0 2000\n",
      "agent1/mean_episode_rewards 0.0 2000\n",
      "agent0/mean_episode_rewards 0.0 2100\n",
      "agent1/mean_episode_rewards 0.0 2100\n",
      "agent0/mean_episode_rewards 0.0 2200\n",
      "agent1/mean_episode_rewards 0.0 2200\n",
      "agent0/mean_episode_rewards 0.0 2300\n",
      "agent1/mean_episode_rewards 0.0 2300\n",
      "agent0/mean_episode_rewards 0.0 2400\n",
      "agent1/mean_episode_rewards 0.0 2400\n",
      "agent0/mean_episode_rewards 0.0 2500\n",
      "agent1/mean_episode_rewards 0.0 2500\n",
      "agent0/mean_episode_rewards 0.0 2600\n",
      "agent1/mean_episode_rewards 0.0 2600\n",
      "agent0/mean_episode_rewards 0.0 2700\n",
      "agent1/mean_episode_rewards 0.0 2700\n",
      "agent0/mean_episode_rewards 0.0 2800\n",
      "agent1/mean_episode_rewards 0.0 2800\n",
      "agent0/mean_episode_rewards 0.0 2900\n",
      "agent1/mean_episode_rewards 0.0 2900\n",
      "agent0/mean_episode_rewards 0.0 3000\n",
      "agent1/mean_episode_rewards 0.0 3000\n",
      "agent0/mean_episode_rewards 0.0 3100\n",
      "agent1/mean_episode_rewards 0.0 3100\n",
      "agent0/mean_episode_rewards 0.0 3200\n",
      "agent1/mean_episode_rewards 0.0 3200\n",
      "agent0/mean_episode_rewards 0.0 3300\n",
      "agent1/mean_episode_rewards 0.0 3300\n",
      "agent0/mean_episode_rewards 0.0 3400\n",
      "agent1/mean_episode_rewards 0.0 3400\n",
      "agent0/mean_episode_rewards 0.0 3500\n",
      "agent1/mean_episode_rewards 0.0 3500\n",
      "agent0/mean_episode_rewards 0.0 3600\n",
      "agent1/mean_episode_rewards 0.0 3600\n",
      "agent0/mean_episode_rewards 0.0 3700\n",
      "agent1/mean_episode_rewards 0.0 3700\n",
      "agent0/mean_episode_rewards 0.0 3800\n",
      "agent1/mean_episode_rewards 0.0 3800\n",
      "agent0/mean_episode_rewards 0.0 3900\n",
      "agent1/mean_episode_rewards 0.0 3900\n",
      "agent0/mean_episode_rewards 0.0 4000\n",
      "agent1/mean_episode_rewards 0.0 4000\n",
      "agent0/mean_episode_rewards 0.0 4100\n",
      "agent1/mean_episode_rewards 0.0 4100\n",
      "agent0/mean_episode_rewards 0.0 4200\n",
      "agent1/mean_episode_rewards 0.0 4200\n",
      "agent0/mean_episode_rewards 0.0 4300\n",
      "agent1/mean_episode_rewards 0.0 4300\n",
      "agent0/mean_episode_rewards 0.0 4400\n",
      "agent1/mean_episode_rewards 0.0 4400\n",
      "agent0/mean_episode_rewards 0.0 4500\n",
      "agent1/mean_episode_rewards 0.0 4500\n",
      "agent0/mean_episode_rewards 0.0 4600\n",
      "agent1/mean_episode_rewards 0.0 4600\n",
      "agent0/mean_episode_rewards 0.0 4700\n",
      "agent1/mean_episode_rewards 0.0 4700\n",
      "agent0/mean_episode_rewards 0.0 4800\n",
      "agent1/mean_episode_rewards 0.0 4800\n",
      "agent0/mean_episode_rewards 0.0 4900\n",
      "agent1/mean_episode_rewards 0.0 4900\n",
      "agent0/mean_episode_rewards 0.0 5000\n",
      "agent1/mean_episode_rewards 0.0 5000\n",
      "agent0/mean_episode_rewards 0.0 5100\n",
      "agent1/mean_episode_rewards 0.0 5100\n",
      "agent0/mean_episode_rewards 0.0 5200\n",
      "agent1/mean_episode_rewards 0.0 5200\n",
      "agent0/mean_episode_rewards 0.0 5300\n",
      "agent1/mean_episode_rewards 0.0 5300\n",
      "agent0/mean_episode_rewards 0.0 5400\n",
      "agent1/mean_episode_rewards 0.0 5400\n",
      "agent0/mean_episode_rewards 0.0 5500\n",
      "agent1/mean_episode_rewards 0.0 5500\n",
      "agent0/mean_episode_rewards 0.0 5600\n",
      "agent1/mean_episode_rewards 0.0 5600\n",
      "agent0/mean_episode_rewards 0.0 5700\n",
      "agent1/mean_episode_rewards 0.0 5700\n",
      "agent0/mean_episode_rewards 0.0 5800\n",
      "agent1/mean_episode_rewards 0.0 5800\n",
      "agent0/mean_episode_rewards 0.0 5900\n",
      "agent1/mean_episode_rewards 0.0 5900\n",
      "agent0/mean_episode_rewards 0.0 6000\n",
      "agent1/mean_episode_rewards 0.0 6000\n",
      "agent0/mean_episode_rewards 0.0 6100\n",
      "agent1/mean_episode_rewards 0.0 6100\n",
      "agent0/mean_episode_rewards 0.0 6200\n",
      "agent1/mean_episode_rewards 0.0 6200\n",
      "agent0/mean_episode_rewards 0.10000000149011612 6300\n",
      "agent1/mean_episode_rewards 0.10000000149011612 6300\n",
      "agent0/mean_episode_rewards 0.10000000149011612 6400\n",
      "agent1/mean_episode_rewards 0.10000000149011612 6400\n",
      "agent0/mean_episode_rewards 0.10000000149011612 6500\n",
      "agent1/mean_episode_rewards 0.10000000149011612 6500\n",
      "agent0/mean_episode_rewards 0.10000000149011612 6600\n",
      "agent1/mean_episode_rewards 0.10000000149011612 6600\n",
      "agent0/mean_episode_rewards 0.10000000149011612 6700\n",
      "agent1/mean_episode_rewards 0.10000000149011612 6700\n",
      "agent0/mean_episode_rewards 0.10000000149011612 6800\n",
      "agent1/mean_episode_rewards 0.10000000149011612 6800\n",
      "agent0/mean_episode_rewards 0.10000000149011612 6900\n",
      "agent1/mean_episode_rewards 0.10000000149011612 6900\n",
      "agent0/mean_episode_rewards 0.10000000149011612 7000\n",
      "agent1/mean_episode_rewards 0.10000000149011612 7000\n",
      "agent0/mean_episode_rewards 0.10000000149011612 7100\n",
      "agent1/mean_episode_rewards 0.10000000149011612 7100\n",
      "agent0/mean_episode_rewards 0.10000000149011612 7200\n",
      "agent1/mean_episode_rewards 0.10000000149011612 7200\n",
      "agent0/mean_episode_rewards 0.10000000149011612 7300\n",
      "agent1/mean_episode_rewards 0.10000000149011612 7300\n",
      "agent0/mean_episode_rewards 0.10000000149011612 7400\n",
      "agent1/mean_episode_rewards 0.10000000149011612 7400\n",
      "agent0/mean_episode_rewards 0.10000000149011612 7500\n",
      "agent1/mean_episode_rewards 0.10000000149011612 7500\n",
      "agent0/mean_episode_rewards 0.20000000298023224 7600\n",
      "agent1/mean_episode_rewards 0.20000000298023224 7600\n",
      "agent0/mean_episode_rewards 0.20000000298023224 7700\n",
      "agent1/mean_episode_rewards 0.20000000298023224 7700\n",
      "agent0/mean_episode_rewards 0.20000000298023224 7800\n",
      "agent1/mean_episode_rewards 0.20000000298023224 7800\n",
      "agent0/mean_episode_rewards 0.20000000298023224 7900\n",
      "agent1/mean_episode_rewards 0.20000000298023224 7900\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8000\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8000\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8100\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8100\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8200\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8200\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8300\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8300\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8400\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8400\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8500\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8500\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8600\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8600\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8700\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8700\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8800\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8800\n",
      "agent0/mean_episode_rewards 0.20000000298023224 8900\n",
      "agent1/mean_episode_rewards 0.20000000298023224 8900\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9000\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9000\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9100\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9100\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9200\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9200\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9300\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9300\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9400\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9400\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9500\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent0/mean_episode_rewards 0.20000000298023224 9600\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9600\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9700\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9700\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9800\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9800\n",
      "agent0/mean_episode_rewards 0.20000000298023224 9900\n",
      "agent1/mean_episode_rewards 0.20000000298023224 9900\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10000\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10000\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10100\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10100\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10200\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10200\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10300\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10300\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10400\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10400\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10500\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10500\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10600\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10600\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10700\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10700\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10800\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10800\n",
      "agent0/mean_episode_rewards 0.20000000298023224 10900\n",
      "agent1/mean_episode_rewards 0.20000000298023224 10900\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11000\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11000\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11100\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11100\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11200\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11200\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11300\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11300\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11400\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11400\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11500\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11500\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11600\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11600\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11700\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11700\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11800\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11800\n",
      "agent0/mean_episode_rewards 0.20000000298023224 11900\n",
      "agent1/mean_episode_rewards 0.20000000298023224 11900\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12000\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12000\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12100\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12100\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12200\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12200\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12300\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12300\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12400\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12400\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12500\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12500\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12600\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12600\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12700\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12700\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12800\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12800\n",
      "agent0/mean_episode_rewards 0.20000000298023224 12900\n",
      "agent1/mean_episode_rewards 0.20000000298023224 12900\n",
      "agent0/mean_episode_rewards 0.20000000298023224 13000\n",
      "agent1/mean_episode_rewards 0.20000000298023224 13000\n",
      "agent0/mean_episode_rewards 0.20000000298023224 13100\n",
      "agent1/mean_episode_rewards 0.20000000298023224 13100\n",
      "agent0/mean_episode_rewards 0.20000000298023224 13200\n",
      "agent1/mean_episode_rewards 0.20000000298023224 13200\n",
      "agent0/mean_episode_rewards 0.20000000298023224 13300\n",
      "agent1/mean_episode_rewards 0.20000000298023224 13300\n",
      "agent0/mean_episode_rewards 0.20000000298023224 13400\n",
      "agent1/mean_episode_rewards 0.20000000298023224 13400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception calling application: Ran out of input\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/oluwaseyiawoga/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/grpc/_server.py\", line 385, in _call_behavior\n",
      "    return behavior(argument, context), True\n",
      "  File \"/Users/oluwaseyiawoga/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/rpc_communicator.py\", line 26, in Exchange\n",
      "    return self.child_conn.recv()\n",
      "  File \"/Users/oluwaseyiawoga/opt/miniconda2/envs/drlnd/lib/python3.6/multiprocessing/connection.py\", line 251, in recv\n",
      "    return _ForkingPickler.loads(buf.getbuffer())\n",
      "EOFError: Ran out of input\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'TennisBrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-33100127a5b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtester\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmainA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-1f58cd1fb4c1>\u001b[0m in \u001b[0;36mmainA\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mactions_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mactions_for_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_for_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mnext_obs_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mnext_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda2/envs/drlnd/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_external_brain_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TennisBrain'"
     ]
    }
   ],
   "source": [
    "tester = mainA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-castle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
